# LLM Sunumu - 5 Temel Kavram + Bonus

## 🎯 ANA KAVRAMLAR (5 Pillar)

### 1. **Next Token Prediction** ✅ (LLM'nin Özü)
*"LLM'ler sadece bir sonraki kelimeyi tahmin eden sistemlerdir"*
- **PDF Bağlantısı:** Sayfa 2-8 (Generation süreci)
- **Sunum Süresi:** 8-10 dakika
- **Demo:** Canlı ChatGPT kelime kelime yazması

---

### 2. **Autoregressive Model** ✅ (LLM'nin Çalışma Şekli)
*"Kendi çıktısını tekrar kendine input olarak verir"*
- **PDF Bağlantısı:** Sayfa 3-4 (Token ekleme döngüsü)
- **Sunum Süresi:** 8-10 dakika  
- **Demo:** "Hikaye anlatma oyunu" canlı demo

---

### 3. **Attention is All You Need** (Transformer'ın Kalbi)
*"Dikkat mekanizması her şeyin anahtarı - hangi kelimelerin önemli olduğunu anlar"*
- **Analoji:** Kokteyl partisinde belirli konuşmalara odaklanmak
- **PDF Bağlantısı:** Sayfa 14-22 (Attention mechanism detayları)
- **Sunum değeri:** Transformer'ın temel prensibi
- **Demo:** "The cat sat on the mat" attention visualization

---

### 4. **In-Context Learning** (Prompting'in Sihri)
*"Parametrelerini değiştirmeden, sadece örnek vererek öğretebilirsiniz"*
- **Günlük analoji:** Arkadaşınıza örnekle anlatmak gibi
- **PDF Bağlantısı:** Sayfa 25-28 (Prompting techniques)
- **Sunum değeri:** Few-shot learning'in gücü
- **Demo:** Zero-shot vs Few-shot comparison

---

### 5. **Foundation Models** (Yeni Bölüm - Versatility)
*"Tek model, binlerce farklı görev - çok amaçlı AI temeli"*
- **Analoji:** İsviçre çakısı gibi
- **Sunum değeri:** Versatility vurgusu
- **Örnekler:** 
  - Aynı GPT-4: Email yazer, kod yazer, çevirir, analiz yapar
  - Specialized model'ler vs Foundation model karşılaştırması
- **Demo:** Bir model, farklı görevler showcase

---

## 🔤 TOKENIZER BÖLÜMÜ

### 6. **Text-to-Numbers, Numbers-to-Text Magic**
*"LLM'ler sayılarla çalışır, ama biz kelimelerle konuşuruz - tokenizer köprü görevi görür"*

**Ana İfade:**
- **Encode:** "Merhaba" → [45, 123, 789] 
- **Decode:** [156, 278, 934] → "dünya"

**Analojiler:**
- **Çevirmen:** İnsan dili ↔ Bilgisayar dili
- **Morse Kodu:** Kelimeler ↔ Nokta/tire
- **Şarkı Notaları:** Müzik ↔ Nota yazısı

**PDF Bağlantısı:** Sayfa 30-34
**Kritik Nokta:** "Tokenization kalitesi = Model performansı"
**Demo:** "Türkçe vs İngilizce tokenization farklılıkları"

---

## 🎁 BONUS KAVRAMLAR (Kısa İfadeler)

### 💪 Fine-Tuning
*"Genel modeli özel göreviniz için eğitebilirsiniz"*
- **Analoji:** Üniversite mezunu profesyonel eğitimi
- **Süreç:** Foundation Model + Your Data = Specialized Model
- **Örnek:** GPT-4 + tıbbi veriler = Tıbbi asistan

### 🧠 Reasoning (Chain of Thought)
*"LLM'lere 'düşünce sürecini göster' dediğinizde daha akıllı olurlar"*
- **Teknik:** Step-by-step thinking
- **Prompt:** "Adım adım düşün..." 
- **Sonuç:** Karmaşık problemleri daha iyi çözer
- **Örnek:** Matematik problemi vs normal çözüm

---

## 🎯 SUNUM AKIŞI ÖNERİSİ

### 📊 Süre Dağılımı (45 dakika total)
1. **Next Token Prediction** (8 dk) - Hook + Temel prensip
2. **Autoregressive Model** (8 dk) - Nasıl çalışır
3. **Attention is All You Need** (12 dk) - Transformer detay
4. **Tokenizer Magic** (5 dk) - Text processing
5. **In-Context Learning** (8 dk) - Prompting power
6. **Foundation Models** (4 dk) - Versatility showcase
7. **Bonus + Q&A** (5 dk) - Fine-tuning, reasoning, sorular

### 🎪 Her Bölüm İçin Demo
1. **Next Token:** ChatGPT kelime kelime yazması
2. **Autoregressive:** Hikaye oyunu (audience participation)
3. **Attention:** Attention weights visualization
4. **Tokenizer:** Türkçe/İngilizce comparison
5. **In-Context:** Few-shot learning demo
6. **Foundation:** Multi-task showcase

### 🔗 PDF Entegrasyonu
- Her kavram için ilgili sayfa referansları
- Görsel diyagramlar kavramları destekler
- Teknik detaylar kavramların üzerine inşa edilir

---

## 💡 GÜÇLÜ SUNUM CÜMLELERİ

### 🔥 Opening Hook
*"LLM'ler rocket science değil! Sadece 5 basit prensiple çalışıyor. Bu 5'ini anlarsanız, tüm LLM dünyasını anlamış olursunuz!"*

### 🎯 Transition Cümleleri
- Next Token → Autoregressive: *"Peki bu tahmin nasıl sürekli hikaye haline geliyor?"*
- Autoregressive → Attention: *"Bu süreç güzel ama hangi kelimelere odaklanacağını nasıl biliyor?"*
- Attention → Tokenizer: *"Kelimeleri anlıyor ama aslında sayılarla çalışıyor!"*
- Tokenizer → In-Context: *"Sayıları çözdük, şimdi modeli nasıl yönlendiriyoruz?"*
- In-Context → Foundation: *"Bu kadar esnek olmasının sırrı ne?"*

### 🎪 Closing Statement
*"5 kavram: Next Token Prediction + Autoregressive + Attention + Tokenizer + In-Context Learning = LLM'lerin tüm sihri! Foundation model'ler bu 5'inin mükemmel kombinasyonu."*

Bu yapı ile hem teknik derinlik hem de anlaşılabilirlik sağlanıyor! 🚀