# LLM Sunumu - 5 Temel Kavram + Bonus

## ğŸ¯ ANA KAVRAMLAR (5 Pillar)

### 1. **Next Token Prediction** âœ… (LLM'nin Ã–zÃ¼)
*"LLM'ler sadece bir sonraki kelimeyi tahmin eden sistemlerdir"*
- **PDF BaÄŸlantÄ±sÄ±:** Sayfa 2-8 (Generation sÃ¼reci)
- **Sunum SÃ¼resi:** 8-10 dakika
- **Demo:** CanlÄ± ChatGPT kelime kelime yazmasÄ±

---

### 2. **Autoregressive Model** âœ… (LLM'nin Ã‡alÄ±ÅŸma Åekli)
*"Kendi Ã§Ä±ktÄ±sÄ±nÄ± tekrar kendine input olarak verir"*
- **PDF BaÄŸlantÄ±sÄ±:** Sayfa 3-4 (Token ekleme dÃ¶ngÃ¼sÃ¼)
- **Sunum SÃ¼resi:** 8-10 dakika  
- **Demo:** "Hikaye anlatma oyunu" canlÄ± demo

---

### 3. **Attention is All You Need** (Transformer'Ä±n Kalbi)
*"Dikkat mekanizmasÄ± her ÅŸeyin anahtarÄ± - hangi kelimelerin Ã¶nemli olduÄŸunu anlar"*
- **Analoji:** Kokteyl partisinde belirli konuÅŸmalara odaklanmak
- **PDF BaÄŸlantÄ±sÄ±:** Sayfa 14-22 (Attention mechanism detaylarÄ±)
- **Sunum deÄŸeri:** Transformer'Ä±n temel prensibi
- **Demo:** "The cat sat on the mat" attention visualization

---

### 4. **In-Context Learning** (Prompting'in Sihri)
*"Parametrelerini deÄŸiÅŸtirmeden, sadece Ã¶rnek vererek Ã¶ÄŸretebilirsiniz"*
- **GÃ¼nlÃ¼k analoji:** ArkadaÅŸÄ±nÄ±za Ã¶rnekle anlatmak gibi
- **PDF BaÄŸlantÄ±sÄ±:** Sayfa 25-28 (Prompting techniques)
- **Sunum deÄŸeri:** Few-shot learning'in gÃ¼cÃ¼
- **Demo:** Zero-shot vs Few-shot comparison

---

### 5. **Foundation Models** (Yeni BÃ¶lÃ¼m - Versatility)
*"Tek model, binlerce farklÄ± gÃ¶rev - Ã§ok amaÃ§lÄ± AI temeli"*
- **Analoji:** Ä°sviÃ§re Ã§akÄ±sÄ± gibi
- **Sunum deÄŸeri:** Versatility vurgusu
- **Ã–rnekler:** 
  - AynÄ± GPT-4: Email yazer, kod yazer, Ã§evirir, analiz yapar
  - Specialized model'ler vs Foundation model karÅŸÄ±laÅŸtÄ±rmasÄ±
- **Demo:** Bir model, farklÄ± gÃ¶revler showcase

---

## ğŸ”¤ TOKENIZER BÃ–LÃœMÃœ

### 6. **Text-to-Numbers, Numbers-to-Text Magic**
*"LLM'ler sayÄ±larla Ã§alÄ±ÅŸÄ±r, ama biz kelimelerle konuÅŸuruz - tokenizer kÃ¶prÃ¼ gÃ¶revi gÃ¶rÃ¼r"*

**Ana Ä°fade:**
- **Encode:** "Merhaba" â†’ [45, 123, 789] 
- **Decode:** [156, 278, 934] â†’ "dÃ¼nya"

**Analojiler:**
- **Ã‡evirmen:** Ä°nsan dili â†” Bilgisayar dili
- **Morse Kodu:** Kelimeler â†” Nokta/tire
- **ÅarkÄ± NotalarÄ±:** MÃ¼zik â†” Nota yazÄ±sÄ±

**PDF BaÄŸlantÄ±sÄ±:** Sayfa 30-34
**Kritik Nokta:** "Tokenization kalitesi = Model performansÄ±"
**Demo:** "TÃ¼rkÃ§e vs Ä°ngilizce tokenization farklÄ±lÄ±klarÄ±"

---

## ğŸ BONUS KAVRAMLAR (KÄ±sa Ä°fadeler)

### ğŸ’ª Fine-Tuning
*"Genel modeli Ã¶zel gÃ¶reviniz iÃ§in eÄŸitebilirsiniz"*
- **Analoji:** Ãœniversite mezunu profesyonel eÄŸitimi
- **SÃ¼reÃ§:** Foundation Model + Your Data = Specialized Model
- **Ã–rnek:** GPT-4 + tÄ±bbi veriler = TÄ±bbi asistan

### ğŸ§  Reasoning (Chain of Thought)
*"LLM'lere 'dÃ¼ÅŸÃ¼nce sÃ¼recini gÃ¶ster' dediÄŸinizde daha akÄ±llÄ± olurlar"*
- **Teknik:** Step-by-step thinking
- **Prompt:** "AdÄ±m adÄ±m dÃ¼ÅŸÃ¼n..." 
- **SonuÃ§:** KarmaÅŸÄ±k problemleri daha iyi Ã§Ã¶zer
- **Ã–rnek:** Matematik problemi vs normal Ã§Ã¶zÃ¼m

---

## ğŸ¯ SUNUM AKIÅI Ã–NERÄ°SÄ°

### ğŸ“Š SÃ¼re DaÄŸÄ±lÄ±mÄ± (45 dakika total)
1. **Next Token Prediction** (8 dk) - Hook + Temel prensip
2. **Autoregressive Model** (8 dk) - NasÄ±l Ã§alÄ±ÅŸÄ±r
3. **Attention is All You Need** (12 dk) - Transformer detay
4. **Tokenizer Magic** (5 dk) - Text processing
5. **In-Context Learning** (8 dk) - Prompting power
6. **Foundation Models** (4 dk) - Versatility showcase
7. **Bonus + Q&A** (5 dk) - Fine-tuning, reasoning, sorular

### ğŸª Her BÃ¶lÃ¼m Ä°Ã§in Demo
1. **Next Token:** ChatGPT kelime kelime yazmasÄ±
2. **Autoregressive:** Hikaye oyunu (audience participation)
3. **Attention:** Attention weights visualization
4. **Tokenizer:** TÃ¼rkÃ§e/Ä°ngilizce comparison
5. **In-Context:** Few-shot learning demo
6. **Foundation:** Multi-task showcase

### ğŸ”— PDF Entegrasyonu
- Her kavram iÃ§in ilgili sayfa referanslarÄ±
- GÃ¶rsel diyagramlar kavramlarÄ± destekler
- Teknik detaylar kavramlarÄ±n Ã¼zerine inÅŸa edilir

---

## ğŸ’¡ GÃœÃ‡LÃœ SUNUM CÃœMLELERÄ°

### ğŸ”¥ Opening Hook
*"LLM'ler rocket science deÄŸil! Sadece 5 basit prensiple Ã§alÄ±ÅŸÄ±yor. Bu 5'ini anlarsanÄ±z, tÃ¼m LLM dÃ¼nyasÄ±nÄ± anlamÄ±ÅŸ olursunuz!"*

### ğŸ¯ Transition CÃ¼mleleri
- Next Token â†’ Autoregressive: *"Peki bu tahmin nasÄ±l sÃ¼rekli hikaye haline geliyor?"*
- Autoregressive â†’ Attention: *"Bu sÃ¼reÃ§ gÃ¼zel ama hangi kelimelere odaklanacaÄŸÄ±nÄ± nasÄ±l biliyor?"*
- Attention â†’ Tokenizer: *"Kelimeleri anlÄ±yor ama aslÄ±nda sayÄ±larla Ã§alÄ±ÅŸÄ±yor!"*
- Tokenizer â†’ In-Context: *"SayÄ±larÄ± Ã§Ã¶zdÃ¼k, ÅŸimdi modeli nasÄ±l yÃ¶nlendiriyoruz?"*
- In-Context â†’ Foundation: *"Bu kadar esnek olmasÄ±nÄ±n sÄ±rrÄ± ne?"*

### ğŸª Closing Statement
*"5 kavram: Next Token Prediction + Autoregressive + Attention + Tokenizer + In-Context Learning = LLM'lerin tÃ¼m sihri! Foundation model'ler bu 5'inin mÃ¼kemmel kombinasyonu."*

Bu yapÄ± ile hem teknik derinlik hem de anlaÅŸÄ±labilirlik saÄŸlanÄ±yor! ğŸš€