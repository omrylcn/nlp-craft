# Algorithms Notes

### Paged Attention

- [Efficient Memory Management for Large Language Model Serving with PagedAttention - article](https://arxiv.org/abs/2309.06180)
- [PagedAttention - blog](https://www.hopsworks.ai/dictionary/pagedattention)

### Flash Attention
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)