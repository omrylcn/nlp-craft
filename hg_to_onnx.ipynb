{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"  # Change this to your desired model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dummy input\n",
    "dummy_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    f\"{model_name}.onnx\",\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['last_hidden_state'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        'last_hidden_state': {0: 'batch_size', 1: 'sequence'}\n",
    "    },\n",
    "    opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = ort.InferenceSession(\"distilbert-base-uncased.onnx\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Output: (1, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input\n",
    "text = \"Hello, world!\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"np\")\n",
    "\n",
    "# Run inference\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: encoded_input.input_ids}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(\"ONNX Model Output:\", ort_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
