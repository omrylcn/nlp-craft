{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenaAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI()\n",
    "client.api_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a electric engineer\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Wnat is electricity?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with \n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.contentAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "About Me\n",
    "Java Backend Developer with experience in Spring Boot, Microservices, Kafka, and Docker.\n",
    "Experienced in designing and developing scalable and secure RESTful APIs. Passionate about\n",
    "optimizing system performance, security, and efficiency. Also experienced in .NET development,\n",
    "building enterprise applications using C# and .NET technologies.\n",
    "Experience\n",
    "Software Developer\n",
    "Revolvind\n",
    "10/2022 - Present\n",
    "Developed and deployed a backend service using Java and Spring Boot, implementing periodic\n",
    "calculations and caching mechanisms with Redis. Enabled real-time data monitoring through a\n",
    "React-based UI, ensuring scalable and efficient data processing.\n",
    "Developed a .NET MAUI Blazor-based desktop application for laser-induced breakdown\n",
    "spectroscopy (LIBS) analysis.\n",
    "Enabled real-time data acquisition from laser and spectrometer devices. Implemented alloy\n",
    "composition analysis and PCA for material classification.\n",
    "Designed and implemented an ERP integration system between Autodesk Vault and IFS using\n",
    ".NET MAUI Blazor and console applications.\n",
    "Java Developer - Mobile Developer\n",
    "Freelancer\n",
    "2024 - Present\n",
    "Developed and maintained RESTful APIs using Spring Boot.\n",
    "Implemented JWT-based authentication and authorization with Spring Security.\n",
    "Designed a real-time data streaming mechanism using Apache Kafka.\n",
    "Optimized database operations using PostgreSQL.\n",
    "Containerized microservices using Docker for seamless deployment and automated CI/CD pipeline\n",
    "using GitHub Actions.\n",
    "Developed a Flutter application for data collection from BLE (Bluetooth Low Energy) devices and\n",
    "transmission to the server.\n",
    "Java Developer\n",
    "Smartin Information Technologies\n",
    "05/2022 - 09/2022\n",
    "Developed middleware services using Spring Boot and Apache Camel, enabling seamless\n",
    "integration between enterprise systems.\n",
    "Implemented message routing, data transformation, and workflow orchestration using Apache\n",
    "Camel's Enterprise Integration Patterns (EIP).\n",
    "Integrated RESTful & SOAP middleware services using TIBCO BusinessWorks (BW).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Sen bir profesyonel editörsün. Metinleri daha profesyonel hale getir.\"\n",
    "            },\n",
    "        {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Bu metni daha profesyonel bir dile çevir: {text}\"\n",
    "            }\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert at analyzing text sentiment.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Text: I love this product! Best purchase ever.\\nAnalysis: Positive sentiment, shows strong enthusiasm and satisfaction\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The analysis identifies high positive sentiment with enthusiasm indicated by exclamation marks and superlative language.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Text: Service was okay, but could be better.\\nAnalysis: Mixed sentiment, shows moderate satisfaction with room for improvement\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The analysis indicates neutral to slightly negative sentiment, with acknowledgment of adequacy but clear desire for improvement.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Text: This restaurant was terrible. Never coming back.\\nAnalysis: Please analyze this text's sentiment.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Örnek analiz sistemi oluşturuyoruz\n",
    "prompt = \"\"\"Aşağıdaki örneklere bakarak yorum analizi yapalım:\n",
    "\n",
    "Yorum: \"Bu telefon tam bir hayal kırıklığı. Pili çok çabuk bitiyor.\"\n",
    "Duygu: Negatif\n",
    "Sebep: Ürün performansından memnuniyetsizlik belirtiliyor ve spesifik bir sorun (pil ömrü) vurgulanıyor.\n",
    "\n",
    "Yorum: \"Fiyatı biraz yüksek ama kalitesi buna değer.\"\n",
    "Duygu: Karışık (Nötr-Pozitif)\n",
    "Sebep: Fiyat konusunda çekince belirtilirken ürün kalitesi takdir ediliyor.\n",
    "\n",
    "Yorum: \"Kesinlikle almalısınız! Uzun zamandır kullanıyorum ve çok memnunum.\"\n",
    "Duygu: Pozitif\n",
    "Sebep: Güçlü tavsiye içeriyor ve uzun süreli memnuniyet belirtiliyor.\n",
    "\n",
    "Şimdi şu yorumu analiz eder misiniz?\n",
    "Yorum: \"Kurulumu kolay ama kullanım kılavuzu çok karışık.\"\n",
    "\n",
    "Lütfen duygu durumunu ve sebebini yukarıdaki örneklerdeki gibi analiz edin.\"\"\"\n",
    "\n",
    "try:\n",
    "    # OpenAI API'sine istek gönderiyoruz\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sen deneyimli bir duygu analizi uzmanısın. Kullanıcı yorumlarını detaylı bir şekilde analiz edebilirsin.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,  # Yaratıcı ama tutarlı yanıtlar için\n",
    "        max_tokens=300    # Yeterli uzunlukta yanıt için\n",
    "    )\n",
    "    \n",
    "    # Yanıtı alıp yazdırıyoruz\n",
    "    print(\"\\nAI Analizi:\")\n",
    "    print(completion.choices[0].message.content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Bir hata oluştu: {e}\")\n",
    "\n",
    "# Fonksiyonu çalıştırıyoruz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Metin işleme, bilgisayar bilimlerinin en önemli çalışma alanlarından biridir\n",
    "ve son dönemde metin verilerinin artışıyla birlikte daha da popüler hale gelmiştir. \n",
    "Ancak yapılan çalışmalar incelendiğinde Türkçe dili için çalışma sayısının \n",
    "yeterli olmadığı görülmüştür ve bu çalışmanın katkılarından biri olarak \n",
    "Türkçe dili tercih edilmiştir. Birçok araştırmacıya göre, metin işleme \n",
    "çalışmalarındaki en önemli nokta kelimelerin temsil ediliş biçimi bir \n",
    "başka ifadeyle kelimelerin sayısal olarak nasıl gösterildiğidir. \n",
    "Bu problemin çözümünde, son zamanlara kadar genellikle frekans bazlı \n",
    "geleneksel yöntemler kullanılmış ancak veri miktarlarındaki büyük \n",
    "artışlardan dolayı bu yöntemler işlevsiz kalmışlardır. Bununla birlikte kelime \n",
    "gömmelerinin (Word Embedding) geliştirilmesi ile birlikte metin işleme \n",
    "çalışmalarında büyük bir ilerleme yaşanmış ve oldukça başarılı sonuçlar \n",
    "elde edilmiştir. Kelime vektörleri ile elde edilen gömmelerin geleneksel \n",
    "yöntemlere karşı bir diğer üstünlüğü ise yakın anlamlı kelimeler arasındaki \n",
    "benzerliklerin de tespit edilebilmesidir. Bu çalışmada, Türkçe Wikipedia makaleleri \n",
    "ile yaklaşık 22 milyon satır ve 528 milyon cümleden oluşan oldukça büyük etiketsiz \n",
    "bir Türkçe derlem üretilmiştir. Bu derlem üzerinde, metin işleme çalışmalarında \n",
    "en çok kullanılan GloVe (Global Vectors), Word2Vec yönteminin iki algoritması \n",
    "CBOW (Continuous Bag of Words) ve Skip-Gram, FastText ve ULMFit (Universal Language Model Fine-Tuning) \n",
    "gibi kelime gömme yöntemleri ile ayrı ayrı kelime vektörleri üretilmiştir. Üretilen vektörler \n",
    "çok sınıflı Türkçe bir veri seti üzerinde sınıflandırma işlemi için kullanılmış başarım \n",
    "değerlerine etkileri incelenmiştir. Sınıflandırma aşamasında ise bir derin sinir ağı \n",
    "mimarisi olan ve metin işleme çalışmalarında sıkça kullanılan LSTM (Long-Short Term Memory) \n",
    "tercih edilmiştir. Çalışmada kullanılan LSTM modelinde 3 katmanlı bir yapı tercih edilmiştir. \n",
    "İlk katmanda 512 nöron ikinci katmanda 64 nöron yerleştirilmiştir. Çalışmanın sonuçları \n",
    "incelendiğinde ise GloVe yöntemi ile %78.30, Skip-Gram ile %80.54, FastText ile %80.94, \n",
    "CBOW ile %81.13 ve ULMFit ile %83.30 doğruluk değerleri elde edilmiştir. \n",
    "Kelime gömmelerinin kullanmasıyla performanslara katkı sağladığı görülmüştür. \n",
    "En başarılı performansı ise yöntemler arasında en yeni yöntem olan ULMFit göstermiştir. \n",
    "Çalışmanın ana katkısı olarak, Türkçe metin sınıflandırma problemi için tüm bu kelime gömme yöntemleri \n",
    "ele alınmış ve sonuçlar kıyaslanmıştır. Kelime gömmeleri, oluşturulan derin öğrenme \n",
    "mimarisi ve özellikle Türkçe için hazırlanmış bir veri seti ile sınanmıştır. Çalışmaların \n",
    "tümünde Python programlama dili ve 3.6.5. sürümü kullanılmıştır.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "Araştırma sonuç metni :{text}\n",
    "\n",
    "Aşağıdaki sorulara metne göre cevap veriniz: \n",
    "- Doğruluk oranları nelerdir ?\n",
    "- Ortalama doğruluk oranı nedir ?\n",
    "- Doğruluk oranında varyans nedir,adım adım hesapla lütfen ?\n",
    "- Doğruluk oranında standart sapma nedir,adım adım hesapla lütfen ?\n",
    "- En yüksek doğruluk oranı hangisidir ?\n",
    "- Toplam kaç nöron kullanılmıştır ?\n",
    "- Neden bu mimari seçilmiştir, bunun nedeni verilen açıklamaya göre nedir ?\n",
    "- Toplam kaç epoch kullanılmıştır ?\n",
    "- Metne göre bir satırda ortalama kaç cümle vardır ?\n",
    "- Eğer bir nöronda yaklaşık 2123 tane parametre varsa, toplam kaç parametre kullanılmıştır ?\n",
    "- Yazarın bahs ettiği doğruluk oranlarını sıra ile denedeğini varsayalım, doğruluk oranlarının her bir denemeden sonra artışı ne olurdu ?\n",
    "- Yazarın bahsettiği sonuçları alfbetik sırayla denedğini varsayalım, doğruluk oranlarının her bir denemeden sonra artışı ne olurdu ?\n",
    "\n",
    "Dikat Edilecek Hususlar : \n",
    "- Kendi yorumunuzu katmayınız ! \n",
    "- Emin olmadığınız sorulara 'emin değilim' diye cevap veriniz.\n",
    "- Hesaplamalarda virgüllden sonra 3 basamak alınız !\n",
    "- Hesaplama işlemleriniz yaklaşık sonuç ise yaklaşık değerleri yazınız, ve yaklaşık diye belirttiğiniz ! \n",
    "- Hesaplama yaparken, sonuçları iki kere kontrol ediniz !\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\": \"Sen uzman bir araştırmacısın, numerik değerler senin çok önemli\"\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "- Doğruluk oranları nelerdir?  \n",
    "  GloVe ile %78.30, Skip-Gram ile %80.54, FastText ile %80.94, CBOW ile %81.13 ve ULMFit ile %83.30.\n",
    "\n",
    "- Ortalama doğruluk oranı nedir?  \n",
    "  Ortalama doğruluk oranı = (78.30 + 80.54 + 80.94 + 81.13 + 83.30) / 5 = 80.842.\n",
    "\n",
    "- Doğruluk oranında varyans nedir?  \n",
    "  Varyans, her bir doğruluk oranının ortalamadan farkının karesinin aritmetik ortalamasıdır.  \n",
    "  ([(78.30 - 80.842)² + (80.54 - 80.842)² + (80.94 - 80.842)² + (81.13 - 80.842)² + (83.30 - 80.842)²] / 5)  \n",
    "  Varyans = [(6.2509 + 0.0916 + 0.0096 + 0.0828 + 6.0596) / 5] = 2.09892.  \n",
    "  Varyans = 2.09892.\n",
    "\n",
    "- Doğruluk oranında standart sapma nedir?  \n",
    "  Standart sapma, varyansın kareköküdür.  \n",
    "  Standart Sapma = √2.09892 ≈ 1.45.\n",
    "\n",
    "- En yüksek doğruluk oranı hangisidir?  \n",
    "  En yüksek doğruluk oranı ULMFit ile %83.30'dur.\n",
    "\n",
    "- Toplam kaç nöron kullanılmıştır?  \n",
    "  Toplam nöron sayısı = 512 (ilk katman) + 64 (ikinci katman) = 576 nöron.\n",
    "\n",
    "- Neden bu mimari seçilmiştir, bunun nedeni verilen açıklamaya göre nedir?  \n",
    "  Metne göre, LSTM mimarisi metin işleme çalışmalarında sıkça kullanıldığı için tercih edilmiştir.\n",
    "\n",
    "- Toplam kaç epoch kullanılmıştır?  \n",
    "  Metinde toplam kaç epoch kullanıldığına dair bilgi verilmemiştir, bu nedenle emin değilim.\n",
    "\n",
    "- Metne göre bir satırda ortalama kaç cümle vardır?  \n",
    "  Metinde toplam 22 milyon satır ve 528 milyon cümle olduğu belirtilmiş. Ortalama cümle sayısı ≈ 528 milyon / 22 milyon = 24 cümle/satır.\n",
    "\n",
    "- Eğer bir nöronda yaklaşık 2123 tane parametre varsa, toplam kaç parametre kullanılmıştır?  \n",
    "  Toplam parametre sayısı = 576 nöron * 2123 parametre/nöron = 1,225,328 parametre.\n",
    "\n",
    "- Yazarın bahsettiği doğruluk oranlarını sıra ile denediğini varsayalım, doğruluk oranlarının her bir denemeden sonra artışı ne olurdu?  \n",
    "  Sırasıyla denendiğinde;  \n",
    "  1. GloVe (%78.30)  \n",
    "  2. Skip-Gram (%80.54) → Artış: 2.24  \n",
    "  3. FastText (%80.94) → Artış: 0.40  \n",
    "  4. CBOW (%81.13) → Artış: 0.19  \n",
    "  5. ULMFit (%83.30) → Artış: 2.17.\n",
    "\n",
    "- Yazarın bahsettiği sonuçları alfabetik sırayla denediğini varsayalım, doğruluk oranlarının her bir denemeden sonra artışı ne olurdu?  \n",
    "  Alfabetik sıraya göre;  \n",
    "  1. CBOW (%81.13)  \n",
    "  2. FastText (%80.94) → Düşüş: -0.19  \n",
    "  3. GloVe (%78.30) → Düşüş: -2.64  \n",
    "  4. Skip-Gram (%80.54) → Artış: 2.24  \n",
    "  5. ULMFit (%83.30) → Artış: 2.76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "# Mevcut modellerden birini kullanarak:\n",
    "def send_request(prompt,model =\"deepseek-r1:1.5b\" ):\n",
    "    response = client.chat(model=model, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"wow !, thats good, are you chat gpt ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"wow !, which chat gpt model ? ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"are you chat-gpt?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"now, write a function to make great thing which you decide\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"your are a standart person\"\n",
    "question = \"how avoid overfitting in ml\"\n",
    "prompt_template = f\"\"\" \n",
    "Context: {context} \n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "r = send_request(prompt_template)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"You're an experienced real senior ml engineer and your job is help people for ml \"\n",
    "question = \"how avoid overfitting in ml\"\n",
    "prompt_template = f\"\"\" \n",
    "Context: {context} \n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "r = send_request(prompt_template)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = send_request(\"what my before question ?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"you are senior ml engineer, can you explain knn algortihms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"you are junior ml engineer , can you explain knn algortihms\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"you are senior ml engineer , can you explain knn algortihms\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"you are elite, expert ml engineer , can you explain knn algortihms\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = send_request(\"can you explain knn algortihms\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
