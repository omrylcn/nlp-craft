# NLP-Works

This repository is a collection of resources and information related to advancements in Natural Language Processing (NLP), with a focus on Large Language Models (LLMs) and parameter-efficient finetuning techniques.

## Parameter-Efficient Finetuning (PEFT)

This section covers the latest developments in optimizing the finetuning process of large language models, aiming to reduce the number of parameters that need to be updated during the training process.

- [Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- [Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)](https://lightning.ai/pages/community/tutorial/lora-llm/)
- [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608)
- [Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)

## Retrieval Augmented Generation (RAG)

This section focuses on the integration of semantic search and retrieval capabilities into the LLM generation process, known as Retrieval Augmented Generation (RAG).

- [langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch/tree/main?tab=readme-ov-file)
- [Tutorial: Building your own retrieval-augmented generation system - llms deep dive](https://github.com/springer-llms-deep-dive/llms-deep-dive-tutorials/tree/main/tutorials/chapter7)
- [Hands-On-Large-Language-Models - Chapter 8 - Semantic Search and Retrieval-Augmented Generation](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/tree/main/chapter08)
- [Building an LLM open source search engine in 100 lines using LangChain and Ray](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)

## Code Repositories

This section links to various code repositories and Jupyter Notebooks that provide practical implementations and examples of the techniques discussed in this README.

- [Low-Rank Adaptation Blog Code](https://github.com/rasbt/low-rank-adaptation-blog/tree/main/code)
- [Blog: Finetuning LLaMA Adapters Code](https://github.com/rasbt/blog-finetuning-llama-adapters/tree/main/three-conventional-methods)
- [LLMs from Scratch Appendix E Notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb)

