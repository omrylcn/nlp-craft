{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two steps :** \n",
    "- create embeding and save \n",
    "- create retrier and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.vector_store import VectorStore\n",
    "from src.config import Config\n",
    "\n",
    "\n",
    "# document_loader.py\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_web_documents(web_paths):\n",
    "        bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=web_paths,\n",
    "            bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "        )\n",
    "        return loader.load()\n",
    "    \n",
    "# document_processor.py\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=2000, chunk_overlap=200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap, \n",
    "            add_start_index=True\n",
    "        )\n",
    "\n",
    "    def split_documents(self, docs):\n",
    "        return self.text_splitter.split_documents(docs)\n",
    "    \n",
    "\n",
    "# llm.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self,config=None):\n",
    "        self.config = config if config else Config()\n",
    "        self.model = ChatOpenAI(model=self.config.MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.config import Config\n",
    "from src.document import DocumentLoader,DocumentProcessor\n",
    "from src.vector_store import VectorStore\n",
    "from src.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/databases/default_database?tenant=default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v1/collections?tenant=default_tenant&database=default_database \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "web_paths = [\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-06-09-vlm/\"\n",
    "]\n",
    "docs = DocumentLoader.load_web_documents(web_paths)\n",
    "\n",
    "\n",
    "# # Process documents\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "split_docs = processor.split_documents(docs)\n",
    "# Create vector store\n",
    "vector_store = VectorStore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/pre-flight-checks \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v1/collections/0c748cc5-3d4e-4701-84c8-d693ac54f10b/upsert \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag import RAG\n",
    "from src.config import Config\n",
    "from src.vector_store import VectorStore\n",
    "from src.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/databases/default_database?tenant=default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v1/collections?tenant=default_tenant&database=default_database \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    " # Initialize vector store\n",
    "vector_store = VectorStore()\n",
    "retriever = vector_store.get_retriever()\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "INFO:src.db:Database initialized successfully\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(retriever,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v1/collections/0c748cc5-3d4e-4701-84c8-d693ac54f10b/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:src.rag:Not enough questions for trend calculation\n",
      "INFO:src.db:Conversation e8e72cc9-b39c-4aad-b692-dfb725eb21b4 saved successfully\n",
      "INFO:src.rag:RAG process completed for question: What is the capital of France?...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.',\n",
       " {'answer': 'The capital of France is Paris.',\n",
       "  'model_used': 'gpt-4o-mini',\n",
       "  'response_time': 1.646196,\n",
       "  'relevance': 'RELEVANT',\n",
       "  'prompt_tokens': 7,\n",
       "  'completion_tokens': 7,\n",
       "  'total_tokens': 14,\n",
       "  'openai_cost': 2.1e-06,\n",
       "  'question_trend': 0.0},\n",
       " 'e8e72cc9-b39c-4aad-b692-dfb725eb21b4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.run(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# db.py\n",
    "import os\n",
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import logging\n",
    "\n",
    "tz = ZoneInfo(\"Europe/Berlin\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_db_connection():\n",
    "    try:\n",
    "        return psycopg2.connect(\n",
    "            host=os.getenv(\"POSTGRES_HOST\", \"postgres\"),\n",
    "            database=os.getenv(\"POSTGRES_DB\", \"course_assistant\"),\n",
    "            user=os.getenv(\"POSTGRES_USER\", \"your_username\"),\n",
    "            password=os.getenv(\"POSTGRES_PASSWORD\", \"your_password\"),\n",
    "        )\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Unable to connect to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "def init_db():\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS conversations (\n",
    "                    id TEXT PRIMARY KEY,\n",
    "                    question TEXT NOT NULL,\n",
    "                    answer TEXT NOT NULL,\n",
    "                    model_used TEXT NOT NULL,\n",
    "                    response_time FLOAT NOT NULL,\n",
    "                    relevance TEXT,\n",
    "                    prompt_tokens INTEGER,\n",
    "                    completion_tokens INTEGER,\n",
    "                    total_tokens INTEGER,\n",
    "                    openai_cost FLOAT,\n",
    "                    question_trend FLOAT,\n",
    "                    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "        conn.commit()\n",
    "        logger.info(\"Database initialized successfully\")\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error initializing database: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def save_conversation(conversation_id, question, answer_data):\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO conversations \n",
    "                (id, question, answer, model_used, response_time, relevance, \n",
    "                prompt_tokens, completion_tokens, total_tokens, openai_cost, question_trend)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                conversation_id,\n",
    "                question,\n",
    "                answer_data['answer'],\n",
    "                answer_data['model_used'],\n",
    "                answer_data['response_time'],\n",
    "                answer_data['relevance'],\n",
    "                answer_data['prompt_tokens'],\n",
    "                answer_data['completion_tokens'],\n",
    "                answer_data['total_tokens'],\n",
    "                answer_data['openai_cost'],\n",
    "                answer_data[\"question_trend\"]\n",
    "            ))\n",
    "        conn.commit()\n",
    "        logger.info(f\"Conversation {conversation_id} saved successfully\")\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error saving conversation: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def get_recent_conversations(limit=5):\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT * FROM conversations\n",
    "                ORDER BY timestamp DESC\n",
    "                LIMIT %s\n",
    "            \"\"\", (limit,))\n",
    "            return cur.fetchall()\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error retrieving recent conversations: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def drop_db():\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS feedback\")\n",
    "            cur.execute(\"DROP TABLE IF EXISTS conversations\")\n",
    "        conn.commit()\n",
    "        logger.info(\"Database dropped successfully\")\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error dropping database: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "#drop_db()\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# from db import save_conversation, get_db_connection, init_db\n",
    "from collections import deque\n",
    "from src.embedding import get_embedding,EmbeddingFactory\n",
    "from src.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, retriever, llm, db_connection=None,last_question_count=20,embedding_type=\"SentenceTransformer\"):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        self.db_connection = db_connection\n",
    "        self.initialize_db()\n",
    "        self.question_queue = deque([],last_question_count)\n",
    "        self.embedding_model = EmbeddingFactory().get_embedding_model(embedding_type)\n",
    "\n",
    "    def initialize_db(self):\n",
    "        init_db()\n",
    "        if self.db_connection is None:\n",
    "            self.db_connection = get_db_connection()\n",
    "\n",
    "    @staticmethod\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def create_chain(self):\n",
    "        return (\n",
    "            {\"context\": self.retriever | self.format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def run(self, question):\n",
    "        try:\n",
    "            chain = self.create_chain()\n",
    "            start_time = datetime.now()\n",
    "            answer = chain.invoke(question)\n",
    "            end_time = datetime.now()\n",
    "            response_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "            self.question_queue.append(self.embedding_model.embed(question))\n",
    "\n",
    "    \n",
    "            # Prepare answer data\n",
    "            answer_data = {\n",
    "                \"answer\": answer,\n",
    "                \"model_used\": self.llm.model_name,\n",
    "                \"response_time\": response_time,\n",
    "                \"relevance\": self.evaluate_relevance(question, answer),\n",
    "                \"prompt_tokens\": self.llm.get_num_tokens(question),\n",
    "                \"completion_tokens\": self.llm.get_num_tokens(answer),\n",
    "                \"total_tokens\": self.llm.get_num_tokens(question) + self.llm.get_num_tokens(answer),\n",
    "                \"openai_cost\": self.calculate_cost(self.llm.get_num_tokens(question), self.llm.get_num_tokens(answer)),\n",
    "                \"question_trend\" : self.analyze_question_trend()\n",
    "            }\n",
    "\n",
    "            # Save conversation\n",
    "            conversation_id = str(uuid.uuid4())\n",
    "            save_conversation(conversation_id, question, answer_data)\n",
    "\n",
    "            logger.info(f\"RAG process completed for question: {question[:50]}...\")\n",
    "            return answer, answer_data, conversation_id\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in RAG process: {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_relevance(self, question, answer):\n",
    "        # Implement relevance evaluation logic here\n",
    "        # This could be a simple heuristic or a more complex model\n",
    "        return \"RELEVANT\"  # Placeholder\n",
    "\n",
    "    def calculate_cost(self, prompt_tokens, completion_tokens):\n",
    "        # Implement cost calculation based on your pricing model\n",
    "        return (prompt_tokens * 0.0001 + completion_tokens * 0.0002) / 1000  # Example calculation\n",
    "\n",
    "    def get_recent_conversations(self, limit=5):\n",
    "        # This method could be used to retrieve recent conversations from the database\n",
    "        from rag.src.db import get_recent_conversations\n",
    "        return get_recent_conversations(limit)\n",
    "    \n",
    "    def analyze_question_trend(self, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculate trend based on the last few question embeddings.\n",
    "        \"\"\"\n",
    "        if len(self.question_queue) < window_size:\n",
    "            logger.warning(\"Not enough questions for trend calculation\")\n",
    "            return 0.0 # Return zero vector if not enough data\n",
    "\n",
    "        recent_embeddings = list(self.question_queue)[-window_size:]\n",
    "        trend = np.mean(recent_embeddings, axis=0)\n",
    "        return float(np.mean(trend))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/api/v1/databases/default_database?tenant=default_tenant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/api/v1/collections?tenant=default_tenant&database=default_database \"HTTP/1.1 200 OK\"\n",
      "/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "INFO:__main__:Database initialized successfully\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "#from db import get_db_connection\n",
    "from dotenv import load_dotenv\n",
    "from src.vector_store import VectorStore\n",
    "from src.llm import LLM\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_connection = get_db_connection()\n",
    "retriever =  VectorStore().get_retriever()\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM().model\n",
    "rag = RAG(retriever, llm, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    print(rag.run(\"what is the meaning of life?\"))\n",
    "    time.sleep(3)\n",
    "#answer, answer_data,conversation_id = rag.run(\"what is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client, HttpClient\n",
    "# Example setup of the client to connect to your chroma server\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x79be71a01210>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Client(\n",
    "    Settings(\n",
    "        chroma_server_host='localhost',\n",
    "        chroma_server_http_port=8000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client,HttpClient\n",
    "from chromadb.config import Settings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(\n",
    "    chroma_server_host= \"locahost\",\n",
    "    chroma_server_http_port= 8000\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"test\",\n",
    "    embedding_function=embeddings,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b54332c7-cfba-4f90-8d50-b946d50278b9',\n",
       " 'bea73e38-f222-41ba-b4d5-a84352e87216']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_0 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_0,\n",
    "    document_1,\n",
    "]\n",
    "  \n",
    "  \n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids,collection_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = client.get_collection(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['b54332c7-cfba-4f90-8d50-b946d50278b9',\n",
       "  'bea73e38-f222-41ba-b4d5-a84352e87216'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['The stock market is down 500 points today due to fears of a recession.',\n",
       "  'I have a bad feeling I am going to get deleted :('],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'news'}, {'source': 'tweet'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name.get(ids=['b54332c7-cfba-4f90-8d50-b946d50278b9',\n",
    " 'bea73e38-f222-41ba-b4d5-a84352e87216'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    def create_embeddings(self, texts):\n",
    "        pass\n",
    "\n",
    "    def storage_embeddings(self, embeddings):\n",
    "        pass\n",
    "\n",
    "    def creat_retriever(self, embeddings):\n",
    "        pass\n",
    "\n",
    "    def create_rag_chain(self):\n",
    "        return (\n",
    "            {\"context\": self.retriever | self.format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def run(self, question):\n",
    "        chain = self.create_rag_chain()\n",
    "        return chain.invoke(question)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "\n",
    "\n",
    "web_paths = [\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-06-09-vlm/\"\n",
    "\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=web_paths,\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(),persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'start_index': 6777}, page_content='Some interesting observations on model hallucination behavior:\\n\\nError rates are higher for rarer entities in the task of biography generation.\\nError rates are higher for facts mentioned later in the generation.\\nUsing retrieval to ground the model generation significantly helps reduce hallucination.\\n\\nWei et al. (2024) proposed an evaluation method for checking long-form factuality in LLMs, named SAFE (Search-Augmented Factuality Evaluator; code). The main difference compared to FActScore is that for each self-contained, atomic fact, SAFE uses a language model as an agent to iteratively issue Google Search queries in a multi-step process and reason about whether the search results support or do not support the fact. In each step, the agent generates a search query based on a given fact to check, as well as previously obtained search results. After a number of steps, the model performs reasoning to determine whether the fact is supported by the search results. According to the experiments, SAFE approach works better than human annotators despite of 20x cheaper: 72% agreement rate with humans and 76% win rate over humans when they disagree.\\n\\nFig. 4. Overview of SAFE for factuality evaluation of long-form LLM generation. (Image source: Wei et al. 2024)\\nThe SAFE evaluation metric is F1 @ K. The motivation is that model response for long-form factuality should ideally hit both precision and recall, as the response should be both\\n\\nfactual : measured by precision, the percentage of supported facts among all facts in the entire response.\\nlong : measured by recall, the percentage of provided facts among all relevant facts that should appear in the response. Therefore we want to consider the number of supported facts up to $K$.\\n\\nGiven the model response $y$, the metric F1 @ K is defined as:')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is hallucination in LLM ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    # | llm\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
