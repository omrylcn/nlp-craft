{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import bs4\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "#from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "web_paths = [\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-06-09-vlm/\",\n",
    "]\n",
    "\n",
    "daft_paths = urls = [\n",
    " \n",
    "     \"https://www.getdaft.io/projects/docs/en/stable/10-min.html\",\n",
    "   \"https://www.getdaft.io/projects/docs/en/stable/user_guide/basic_concepts.html\",\n",
    "    \"https://www.getdaft.io/projects/docs/en/stable/user_guide/read-and-write.html\",\n",
    "    \"https://www.getdaft.io/projects/docs/en/stable/user_guide/expressions.html\",\n",
    "    \"https://www.getdaft.io/projects/docs/en/stable/user_guide/datatypes.html\",\n",
    "    \"https://www.getdaft.io/projects/docs/en/stable/user_guide/dataframe-operations.html\",\n",
    "    \"https://www.getdaft.io/projects/docs/en/stable/user_guide/sql.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/aggregations.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/udf.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/memory.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/partitioning.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/distributed-computing.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/ray.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/unity-catalog.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/iceberg.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/delta_lake.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/hudi.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/microsoft-azure.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/aws.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/sql.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/integrations/huggingface.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/fotw/index.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/fotw/fotw-000-data-access.html\",\n",
    "#     \"https://www.getdaft.io/projects/docs/en/stable/user_guide/fotw/fotw-001-images.html\",\n",
    "]\n",
    "\n",
    "\n",
    "# bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"bd-article\",\"bd-main\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=daft_paths[:],\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4_strainer = bs4.SoupStrainer(class_=(\"bd-article\",\"bd-main\"))\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=[\"https://www.getdaft.io/projects/docs/en/stable/10-min.html\",\n",
    "#                \"https://www.getdaft.io/projects/docs/en/stable/\",\n",
    "#                \"https://www.getdaft.io/projects/docs/en/stable/user_guide/basic_concepts.html\",\n",
    "#                ],\n",
    "#     bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "# )\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=100, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OllamaEmbeddings(model=\"llama3.2:1b\"),persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:1b\",temperature=0)\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# 1. Define the prompt template\n",
    "rag_prompt_template = \"\"\"You are a helpful assistant specializing in the Daft data processing library. You have access to official Daft documentation. Use the following pieces of documentation to answer the user's question. If you don't know the answer, just say you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Given this context, please:\n",
    "1. Provide accurate information based on the Daft documentation\n",
    "2. Include relevant code examples when appropriate\n",
    "3. Cite specific sections of documentation you're referencing\n",
    "4. If multiple approaches exist, explain the trade-offs\n",
    "\n",
    "User Question: {question}\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is daft library\"\n",
    "\n",
    "res =rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Retriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi = MultiQueryRetriever.from_llm(\n",
    "    retriever = retriever,\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_multi.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_ret_multi = (\n",
    "    {\"context\": retriever_multi | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    # | llm\n",
    "    # | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag_chain_ret_multi.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag_chain_ret_multi.invoke(\"what is daft index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Multi-query generation prompt\n",
    "multi_query_template = \"\"\"You are an expert in the Daft data processing library. Generate different versions of the given question to retrieve relevant documentation.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Generate 4 different search queries that will help find relevant information from the Daft documentation. The queries should:\n",
    "- Rephrase the original question in different ways\n",
    "- Include technical terms related to Daft\n",
    "- Consider different aspects of the question\n",
    "- Be specific to data processing and Daft's features\n",
    "\n",
    "Output format - just the queries, one per line:\n",
    "1. [Query 1]\n",
    "2. [Query 2]\n",
    "3. [Query 3]\n",
    "4. [Query 4]\n",
    "\n",
    "Generated Queries:\"\"\"\n",
    "\n",
    "multi_query_prompt = PromptTemplate(\n",
    "    template=multi_query_template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "ou are an expert in the Daft data processing library. Generate different versions of the given question to retrieve relevant documentation.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Generate 4 different search queries that will help find relevant information from the Daft documentation. The queries should:\n",
    "- Rephrase the original question in different ways\n",
    "- Include technical terms related to Daft\n",
    "- Consider different aspects of the question\n",
    "- Be specific to data processing and Daft's features\n",
    "\n",
    "Output format - just the queries, one per line:\n",
    "1. [Query 1]\n",
    "2. [Query 2]\n",
    "3. [Query 3]\n",
    "4. [Query 4]\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0) # lmm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries.invoke(\"is daft written in rust ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "#    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "\n",
    "# from langchain_core.output_parsers import BaseOutputParser\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# # Output parser will split the LLM result into a list of queries\n",
    "# class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "#     \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "#     def parse(self, text: str) -> List[str]:\n",
    "#         lines = text.strip().split(\"\\n\")\n",
    "#         return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "# output_parser = LineListOutputParser()\n",
    "\n",
    "# QUERY_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "#     different versions of the given user question to retrieve relevant documents from a vector \n",
    "#     database. By generating multiple perspectives on the user question, your goal is to help\n",
    "#     the user overcome some of the limitations of the distance-based similarity search. \n",
    "#     Provide these alternative questions separated by newlines.\n",
    "#     Original question: {question}\"\"\",\n",
    "# )\n",
    "\n",
    "\n",
    "# # Chain\n",
    "# llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# # Other inputs\n",
    "# question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
