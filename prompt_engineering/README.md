## Prompt Engineering

### Parameters 
- Temperature is a number that controls the randomness of an LLMâ€™s outputs. Most APIs limit the value to be from 0 to 1 or some similar range to keep the outputs in semantically coherent bounds.

- The LLM temperature serves as a critical parameter influencing the balance between predictability and creativity in generated text. Lower temperatures prioritize exploiting learned patterns, yielding more deterministic outputs, while higher temperatures encourage exploration, fostering diversity and innovation.

- [LLM Temperature](https://www.hopsworks.ai/dictionary/llm-temperature#:~:text=The%20LLM%20temperature%20serves%20as,exploration%2C%20fostering%20diversity%20and%20innovation.)
- [A Comprehensive Guide to LLM Temperature - blog](https://towardsdatascience.com/a-comprehensive-guide-to-llm-temperature/)
- [Mastering LLM Parameters: A Deep Dive into Temperature, Top-K, and Top-P](https://plainenglish.io/blog/mastering-llm-parameters-a-deep-dive-into-temperature-top-k-and-top-p)


### Article 
- [2201 - Chain of Thought Prompting Elicits - v6](https://arxiv.org/abs/2201.11903)

- [Understanding Reasoning LLMs - blog](https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html)